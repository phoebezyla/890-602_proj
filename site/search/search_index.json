{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HAWC Map-Making For more information check the HAWC wiki and document database. Scripts themselves are hosted on github, along with cut files. Parameter files and data files are only on the UMD cluster. Scripts More information in about page make-sub-scripts.py - Create bash scripts to generate raw .fits.gz files combine-raw-files.py - Create bash scripts to combine raw .fits.gz files make-histograms.py - Create bash scripts to generate .root histograms make-randomized-bkg.py - Create bash scripts to generate .fits.gz files with randomized background","title":"Home"},{"location":"#hawc-map-making","text":"For more information check the HAWC wiki and document database. Scripts themselves are hosted on github, along with cut files. Parameter files and data files are only on the UMD cluster.","title":"HAWC Map-Making"},{"location":"#scripts","text":"More information in about page make-sub-scripts.py - Create bash scripts to generate raw .fits.gz files combine-raw-files.py - Create bash scripts to combine raw .fits.gz files make-histograms.py - Create bash scripts to generate .root histograms make-randomized-bkg.py - Create bash scripts to generate .fits.gz files with randomized background","title":"Scripts"},{"location":"about/","text":"About This is a project intended to speed up / ease the process to creating maps with HAWC data. In order to follow this process, you need to have an account on the UMD cluster and AERIE installed -- check the software-help slack channel if needed. There is some documentation on the HAWC wiki as well. Cuts files are on github and can be found on UMD cluster. .fits.gz data files are only on cluster -- too big to host elsewhere. These are the scripts and fhit cut files to make fhit maps (including background estimation). The scripts assume that the chunks are already made, and the $CONFIG_HAWC parameter is correctly set. Much of this repository has been modified from Kelly Malone's ground parameter map-making repository (https://gitlab.com/hawc-observatory/analysis-scripts/map-making/-/tree/master/counts-maps/GP-scripts?ref_type=heads). SCRIPT DESCRIPTIONS make-sub-scripts.py Creats job submission scripts for slurm to make the raw maps, broken into a reasonable number of jobs to reduce cluster memory usage. Need to specify aerie location, path to input .txt files, chunks to iterate over, parameter/zenith alignment file, cut file, and output directory. Note: There is one input text file for every chunk. Each text file has, on every line, the path to one .xcd file for that chunk. Each chunk has a different number of .xcd files. combine-raw-files.py Creats job submission scripts to make one file for each bin, using aerie-apps-combine-maps . Specify the directory to the raw maps, a list of the energy bins to iterate over, aerie location, and output directory. make-histograms.py Creates job submission scripts to begin the process of randomizing background for low-statistic bins. For the fhit energy estimator, this is for bins B7C1, B8C0, B8C1, B9C0, B9C1, B10C0, and B10C1. Specify aerie location, path to input .txt files, chunks to iterate over, cut file, and output directory. Note: Chunks to iterate over needs to be the same as those used in make-sub-scripts.py . Otherwise the background randomization will fail because the histogram and map will have different numbers of evernts. make-randomized-bkg.py Creates job submission scripts to randomize the background for the bins with low statistics. Specify aerie location, location of histograms from make-histograms.py , location of combined files from combine-raw-files.py , list of bins to iterate over, parameter/zenith alignment file, and output directory. MAKING MAPS For the fHit energy estimator, there are four groups of energy bins, each of which requires a unique combonation of input .txt files, cut file, parameter/zenith alignment file, and background estimation scheme. Listed below is each group of bins, and their required inputs. B0C0 B0C1 B1C0 B1C1 .../pass5_allbin/chunk%d.txt The .xcd files must come from an allbin directory .../fNhit_100pct_pgamma_mlp_cut_0-1.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 1.2 --dtMax_hr 2.0 --nSide 1024 --roi --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file Background Est. = Direct Integration (DI) B2C0 B2C1 B3C0 B3C1 B4C0 B4C1 B5C0 B5C1 ../pass5_bin2up/chunk%d.txt The .xcd files must come from a bin2up directory .../fNhit_100pct_pgamma_mlp_cut_2-5.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 1.2 --dtMax_hr 2.0 --nSide 1024 --roi --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file Background Est. = Direct Integration (DI) B6C0 B6C1 B7C0 .../pass5_bin6up/chunk%06d.txt The .xcd files must come from a bin6up directory. .../fNhit_100pct_pgamma_mlp_cut_6-7.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 1.2 --dtMax_hr 2.0 --nSide 1024 --roi --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file Background Est. = Direct Integration (DI) B7C1 B8C0 B8C1 B9C0 B9C1 B10C0 B10C1 .../pass5_bin6up/chunk%06d.txt The .xcd files must come from a bin6up directory. .../fNhit_100pct_pgamma_mlp_cut_7up_noDI.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 0 --dtMax_hr 2.0 --nSide 1024 --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file for no DI. Not that dtMin here is 0 -- only those bins with DI have dtMin = 1.2 . Also, the --roi flag is not used for this bin group. Background Est. = Randomized Background In the cuts folder in this repository, all individual cut files and the master cut file are stored. Modify and run make-sub-scripts.py . The script must be run four times, once with the inputs changed for each of the bin groups. Be sure to change the name for the output job submission script The output will be four job submission .sh files per chunk, one for each bin group. Submit job scripts from step 1. Output is one .fits.gz file per chunk per bin Modify and run combine-raw-files.py The script only needs to be run once. The output will be one job submission script per bin Submit job scripts from step 3. Output is one .fits.gz file per bin Modify and run make-histograms.py The script needs to be run once, with the input directory being the directory for the randomize background bin group. Histograms only need to be made for the group w/ energy estimator = randomize background. The output will be one job submission script per chunk Submit job scripts from step 5. Output is one .root file per chunk per bin Combine .root files from (9) with hadd For bin X, do hadd binX.root distsXroot Output is one .root file per bin Modify and run make-randomized-bkg.py Output is one job submission script per bin Submit job scripts from step 10. Output is one -randomized.fits.gz file per bin Replace combined files for randomized background bins with the files from (11). Rejigger all files by an additional 0.5 degrees aerie-apps-recalculate-bkg -i inputfile.fits.gz -o outputfile.fits.gz --smooth 0.5 --nthreads 10 Input files are .fits.gz files from step 5 for all DI bin groups and from step 12 for all randBkg bin groups Output is rejiggered .fits.gz files Last Edited: Nov 2025 P. Zyla","title":"About"},{"location":"about/#about","text":"This is a project intended to speed up / ease the process to creating maps with HAWC data. In order to follow this process, you need to have an account on the UMD cluster and AERIE installed -- check the software-help slack channel if needed. There is some documentation on the HAWC wiki as well. Cuts files are on github and can be found on UMD cluster. .fits.gz data files are only on cluster -- too big to host elsewhere. These are the scripts and fhit cut files to make fhit maps (including background estimation). The scripts assume that the chunks are already made, and the $CONFIG_HAWC parameter is correctly set. Much of this repository has been modified from Kelly Malone's ground parameter map-making repository (https://gitlab.com/hawc-observatory/analysis-scripts/map-making/-/tree/master/counts-maps/GP-scripts?ref_type=heads).","title":"About"},{"location":"about/#script-descriptions","text":"","title":"SCRIPT DESCRIPTIONS"},{"location":"about/#make-sub-scriptspy","text":"Creats job submission scripts for slurm to make the raw maps, broken into a reasonable number of jobs to reduce cluster memory usage. Need to specify aerie location, path to input .txt files, chunks to iterate over, parameter/zenith alignment file, cut file, and output directory. Note: There is one input text file for every chunk. Each text file has, on every line, the path to one .xcd file for that chunk. Each chunk has a different number of .xcd files.","title":"make-sub-scripts.py"},{"location":"about/#combine-raw-filespy","text":"Creats job submission scripts to make one file for each bin, using aerie-apps-combine-maps . Specify the directory to the raw maps, a list of the energy bins to iterate over, aerie location, and output directory.","title":"combine-raw-files.py"},{"location":"about/#make-histogramspy","text":"Creates job submission scripts to begin the process of randomizing background for low-statistic bins. For the fhit energy estimator, this is for bins B7C1, B8C0, B8C1, B9C0, B9C1, B10C0, and B10C1. Specify aerie location, path to input .txt files, chunks to iterate over, cut file, and output directory. Note: Chunks to iterate over needs to be the same as those used in make-sub-scripts.py . Otherwise the background randomization will fail because the histogram and map will have different numbers of evernts.","title":"make-histograms.py"},{"location":"about/#make-randomized-bkgpy","text":"Creates job submission scripts to randomize the background for the bins with low statistics. Specify aerie location, location of histograms from make-histograms.py , location of combined files from combine-raw-files.py , list of bins to iterate over, parameter/zenith alignment file, and output directory.","title":"make-randomized-bkg.py"},{"location":"about/#making-maps","text":"For the fHit energy estimator, there are four groups of energy bins, each of which requires a unique combonation of input .txt files, cut file, parameter/zenith alignment file, and background estimation scheme. Listed below is each group of bins, and their required inputs. B0C0 B0C1 B1C0 B1C1 .../pass5_allbin/chunk%d.txt The .xcd files must come from an allbin directory .../fNhit_100pct_pgamma_mlp_cut_0-1.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 1.2 --dtMax_hr 2.0 --nSide 1024 --roi --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file Background Est. = Direct Integration (DI) B2C0 B2C1 B3C0 B3C1 B4C0 B4C1 B5C0 B5C1 ../pass5_bin2up/chunk%d.txt The .xcd files must come from a bin2up directory .../fNhit_100pct_pgamma_mlp_cut_2-5.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 1.2 --dtMax_hr 2.0 --nSide 1024 --roi --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file Background Est. = Direct Integration (DI) B6C0 B6C1 B7C0 .../pass5_bin6up/chunk%06d.txt The .xcd files must come from a bin6up directory. .../fNhit_100pct_pgamma_mlp_cut_6-7.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 1.2 --dtMax_hr 2.0 --nSide 1024 --roi --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file Background Est. = Direct Integration (DI) B7C1 B8C0 B8C1 B9C0 B9C1 B10C0 B10C1 .../pass5_bin6up/chunk%06d.txt The .xcd files must come from a bin6up directory. .../fNhit_100pct_pgamma_mlp_cut_7up_noDI.txt '-z $CONFIG_HAWC/reconstruction/alignment/align_pass5_v1.0/zenith-pass5-2021-08-13.xml --dtMin 0 --dtMax_hr 2.0 --nSide 1024 --useJ2000 --rndSmear' This is the current (as of 11/2025) zenith alignment file for no DI. Not that dtMin here is 0 -- only those bins with DI have dtMin = 1.2 . Also, the --roi flag is not used for this bin group. Background Est. = Randomized Background In the cuts folder in this repository, all individual cut files and the master cut file are stored. Modify and run make-sub-scripts.py . The script must be run four times, once with the inputs changed for each of the bin groups. Be sure to change the name for the output job submission script The output will be four job submission .sh files per chunk, one for each bin group. Submit job scripts from step 1. Output is one .fits.gz file per chunk per bin Modify and run combine-raw-files.py The script only needs to be run once. The output will be one job submission script per bin Submit job scripts from step 3. Output is one .fits.gz file per bin Modify and run make-histograms.py The script needs to be run once, with the input directory being the directory for the randomize background bin group. Histograms only need to be made for the group w/ energy estimator = randomize background. The output will be one job submission script per chunk Submit job scripts from step 5. Output is one .root file per chunk per bin Combine .root files from (9) with hadd For bin X, do hadd binX.root distsXroot Output is one .root file per bin Modify and run make-randomized-bkg.py Output is one job submission script per bin Submit job scripts from step 10. Output is one -randomized.fits.gz file per bin Replace combined files for randomized background bins with the files from (11). Rejigger all files by an additional 0.5 degrees aerie-apps-recalculate-bkg -i inputfile.fits.gz -o outputfile.fits.gz --smooth 0.5 --nthreads 10 Input files are .fits.gz files from step 5 for all DI bin groups and from step 12 for all randBkg bin groups Output is rejiggered .fits.gz files Last Edited: Nov 2025 P. Zyla","title":"MAKING MAPS"},{"location":"tests/","text":"Tests The AERIE functions within each .py script ( aerie-apps-combine-maps , aerie-apps-make-local-dists , aerie-apps-randomize-bkg , and aerie-apps-make-hawc-maps ) are tested by the AERIE developers before being published. If interested in the tests/AERIE development, please reach out to the following slack channels: # softwareworkshop, # software-help, or # analysis-chat. To test your own I/O and installation process, please interactively run testing.py . It will check the availability of SLURM, an AERIE environment, and whether your produced files match the produced example files in the example folder. You will need to make sure that testing.py points to the yout produced files. NOTE: HAWC's data is not ready for public relesae, so only the files created by the .py scripts are available in this repository. For the purpose of proving that these tests work (for CMSE proj), I have run the process twice on different chunks of data. One chunk was output to the example folder (for others to check against) and one chunk to the test-data folder (to show for class that testing.py works / can tell that these were created with different chunks)","title":"Tests"},{"location":"tests/#tests","text":"The AERIE functions within each .py script ( aerie-apps-combine-maps , aerie-apps-make-local-dists , aerie-apps-randomize-bkg , and aerie-apps-make-hawc-maps ) are tested by the AERIE developers before being published. If interested in the tests/AERIE development, please reach out to the following slack channels: # softwareworkshop, # software-help, or # analysis-chat. To test your own I/O and installation process, please interactively run testing.py . It will check the availability of SLURM, an AERIE environment, and whether your produced files match the produced example files in the example folder. You will need to make sure that testing.py points to the yout produced files. NOTE: HAWC's data is not ready for public relesae, so only the files created by the .py scripts are available in this repository. For the purpose of proving that these tests work (for CMSE proj), I have run the process twice on different chunks of data. One chunk was output to the example folder (for others to check against) and one chunk to the test-data folder (to show for class that testing.py works / can tell that these were created with different chunks)","title":"Tests"}]}